{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_val_predict\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import re\n",
    "from scipy.stats.stats import pearsonr   \n",
    "import pandas as pd\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_anger_en_train = pd.read_csv(\"data/en/EI-reg-en_anger_train.txt\", header=None, names=[\"text\", \"emotion\", \"intensity\"], sep=\"\t\")\n",
    "df_anger_en_dev = pd.read_csv(\"data/en/2018-EI-reg-En-anger-dev.txt\", header=None, names=[\"text\", \"emotion\", \"intensity\"], sep=\"\t\")\n",
    "df_anger_es_train = pd.read_csv(\"data/es/2018-EI-reg-Es-anger-train.txt\", header=None, names=[\"text\", \"emotion\", \"intensity\"], sep=\"\t\")\n",
    "df_anger_es_dev = pd.read_csv(\"data/es/2018-EI-reg-Es-anger-dev.txt\", header=None, names=[\"text\", \"emotion\", \"intensity\"], sep=\"\t\")\n",
    "\n",
    "df_anger_en = df_anger_en_train.append([df_anger_en_dev])\n",
    "df_anger_es = df_anger_es_train.append([df_anger_es_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_anger_en[\"emotion\"] = pd.Categorical(df_anger_en[\"emotion\"]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-191</th>\n",
       "      <td>+ cant get to tell them what will offend them ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-192</th>\n",
       "      <td>Seriously about to smack someone in the face ðŸ˜µ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-193</th>\n",
       "      <td>Been at work for not even 4 hours and I've thr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-194</th>\n",
       "      <td>I click on download on my PC. Message says 'Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-195</th>\n",
       "      <td>Why does @DANCEonFOX get rudely interrupted by...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-196</th>\n",
       "      <td>If I have to hear one more time how I am intim...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-197</th>\n",
       "      <td>i've been to 1 and 1/3 of my classes today and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-198</th>\n",
       "      <td>quick note about insta stories how the f do yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-199</th>\n",
       "      <td>@AliceT120 @shaney_waney1 @shaney_waney1 u hor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-en-anger-dev-200</th>\n",
       "      <td>#Twitter these days is just a massive critical...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    text  \\\n",
       "2018-en-anger-dev-191  + cant get to tell them what will offend them ...   \n",
       "2018-en-anger-dev-192  Seriously about to smack someone in the face ðŸ˜µ...   \n",
       "2018-en-anger-dev-193  Been at work for not even 4 hours and I've thr...   \n",
       "2018-en-anger-dev-194  I click on download on my PC. Message says 'Th...   \n",
       "2018-en-anger-dev-195  Why does @DANCEonFOX get rudely interrupted by...   \n",
       "2018-en-anger-dev-196  If I have to hear one more time how I am intim...   \n",
       "2018-en-anger-dev-197  i've been to 1 and 1/3 of my classes today and...   \n",
       "2018-en-anger-dev-198  quick note about insta stories how the f do yo...   \n",
       "2018-en-anger-dev-199  @AliceT120 @shaney_waney1 @shaney_waney1 u hor...   \n",
       "2018-en-anger-dev-200  #Twitter these days is just a massive critical...   \n",
       "\n",
       "                       emotion  intensity  \n",
       "2018-en-anger-dev-191        0      0.290  \n",
       "2018-en-anger-dev-192        0      0.922  \n",
       "2018-en-anger-dev-193        0      0.833  \n",
       "2018-en-anger-dev-194        0      0.859  \n",
       "2018-en-anger-dev-195        0      0.875  \n",
       "2018-en-anger-dev-196        0      0.766  \n",
       "2018-en-anger-dev-197        0      0.383  \n",
       "2018-en-anger-dev-198        0      0.500  \n",
       "2018-en-anger-dev-199        0      0.750  \n",
       "2018-en-anger-dev-200        0      0.719  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anger_en.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.49672225351908889, 5.0334462759154037e-31)\n",
      "[[ 1.          0.49672225]\n",
      " [ 0.49672225  1.        ]]\n",
      "(0.47509418260514341, 1.7598213856502596e-15)\n",
      "[[ 1.          0.47509418]\n",
      " [ 0.47509418  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline function\n",
    "def use_pipeline(X, y, pipeline):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(pearsonr(y_test, y_pred))\n",
    "    print(np.corrcoef(y_test, y_pred))\n",
    "    \n",
    "\n",
    "X_en = df_anger_en[\"text\"]\n",
    "y_en = df_anger_en[\"intensity\"]\n",
    "X_es = df_anger_es[\"text\"]\n",
    "y_es = df_anger_es[\"intensity\"]\n",
    "pipeline1 = Pipeline([\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer(use_idf=True)),\n",
    "        ('classifier', LinearRegression())\n",
    "    ])\n",
    "# baseline for English - Anger\n",
    "use_pipeline(X_en, y_en, pipeline1)\n",
    "# baseline for Spanish - Anger\n",
    "use_pipeline(X_es, y_es, pipeline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x21dabde6be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English word2vec pre-trained word embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x21dabde6780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spanish word2vec pre-trained word embeddings\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('sbw_vectors.bin', binary=True)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-15 18:13:37,873 : INFO : loading projection weights from wiki.ar.bin\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-dfeb4bf4cb59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# iemand een oplossing voor deze UnicodeDecodeError????\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Arabic word2vec pre-trained word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki.ar.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[0mto_unicode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0many2unicode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# iemand een oplossing voor deze UnicodeDecodeError????\n",
    "# Arabic word2vec pre-trained word embeddings\n",
    "model3 = gensim.models.KeyedVectors.load_word2vec_format('wiki.ar.bin', binary=True, unicode_errors=\"replace\")\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model2.most_similar(positive=['mujer', 'rey'], negative=['hombre']))\n",
    "print(model2.similarity('mujer', 'hombre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "# THE EMOTION PIPELINE BELOW IS ONLY FOR REFERENCE > you can use it to create new features based on this\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        # change 'emotion' to key you want to use\n",
    "        return [{'emotion': text}\n",
    "                for text in posts]\n",
    "    \n",
    "# pipeline that incorporates both text and emotion features\n",
    "pipeline2 = Pipeline([\n",
    "\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=50)),\n",
    "            ])),\n",
    "\n",
    "            ('emotion', Pipeline([\n",
    "                ('selector', ItemSelector(key='emotion')),\n",
    "                ('stats', ItemExtractor()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "        ],\n",
    "\n",
    "        # give equal weights to text and emotion features\n",
    "        transformer_weights={\n",
    "            'text': 1.0,\n",
    "            'emotion': 0.0,\n",
    "            },\n",
    "        )),\n",
    "        ('classifier', LinearRegression())\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
