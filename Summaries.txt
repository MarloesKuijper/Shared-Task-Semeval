Summaries

WASSA-2017 Shared Task on Emotion Intensity
	Report on a previous competition involving the detection of Emotion Intensity from tweets.
	
	Details 
	Set-up of competition: Codalabs was used as the platform. Given a tweet and an emotion X, automatic systems have to determine the intensity or degree of emotion X felt by the speaker—a real-valued score between 0 and 1.

	Creation of the datasets: Emotion Intensity annotation scheme was Best-Worst Scaling (BWS). For each emotion 50-100 query terms were used as indicators of different intensity levels. These query words were derived from a thesaurus.
	
	Baseline system
	Used the package Affective Tweets to calculate feature vectors from the emotion-intensity-labeled tweets and train Weka regression models on this transformed data. 
	Features used: word and character N-grams, word embeddings (word2vec used), affect lexicons (several of them).
	
	Summaries are given on the performance of the contestants: the best performing system, Prayas, obtained a Pearson correlation of 0.747 with the gold annotations.
	
	The top ranking systems used ensembles of multiple models.
	
	Keras was the most widely used ready-made package, using TensorFlow as its base.
	
	Commonly used features for the best performers were low-dimensional distributed word/sentence representations: word embeddings (word2vec for the top 4) and sentence embeddings, the latter learned using neural networks (CNN and LSTM for the top 2).
	The top teams also used affect lexicons, and more of them than less well-performing teams.
	
	Top 3:
	Prayas
	IMS
	SeerNet
	
	Did not use WEKA: Nr.4 : Uwaterloo
	
SeerNet: Duppada & Hiray 2017

	Came in 3rd for Task 1 at Wassa/EMoInt 2017
	
	System
	Tweets are pre-processed with tweetokenize, replacing some user-specific information to general tokens (USERNAME, PHONENUMBER etc.).
	
	Features (total of 14):
		Lexicons: 
		AFINN: words manually rated for valence
		Bing Liu: Opinion lexicon
		+/-EffectWordNet: sense level lexicon
		NRC Affect Intensity: real valued affect intensity
		NRC Word-Emotion Association Lexicon: 8 sense level associations + 2 sentiment level associations (negative and positive)
		Expanded NRC Word-Emotion Association Lexicon: same as above, but specified for twitter language
		NRC Hashtag Emotion Lexicon: emotion word associations computed on emotion labeled twitter corpus via Hashtags
		NRC Hashtag Sentiment Lexicon + Sentiment140 Lexicon: sentiment word associations computed on twitter corpus via Hashtags and Emoticons
		SentiWordNet: sentiment scores of synsets of WordNet
		Negation lexicons: to count negative words
		Sentistrength: to estimate strength of positive and negative sentiment in tweets
		
		Word Vectors:
		Primarily word embeddings created using the dataset and GloVe (unsupervised algorithm for vector represenatations for words. Result is 200-dimensional GloVe embeddings.
		Edinburgh Embeddings: skip-gram model trained on Edinburgh corpus
		Emoji Embeddings:  obtained for each tweet by summing up individual word vectors and then dividing by the number of tokens in that tweet.
		
		Syntactic Features ( NOT used in final system)
		Word N-grams
		Part-Of-Speech N-grams
		Brown Cluster N-grams: obtained with TweetNLP
		
		Final feature: concatenation of all individual features. combination of average word vectors,  sum of NRC affect intensities etc. 
		
	Regression
	Train and development sets merged (dev set was small).
	10-fold cross validation
	Regressors:
		Support Vector Regression
		AdaBoost
		RandomForestRegressor
		BaggingRegressor (sklearn)
		
	Top models chosen by comparing Pearson’s Correlation Coefficient and Spearman’s rank-order correlation
	
	Parameter optimization: 'Extensive' grid search using scikit-Learn
	
	Results:
	Syntactic features (N-grams) dropped from system (did not perform well)
	Emoji embeddings performed better than GloVe or Edinburgh embeddings
	
	Features thought to be most important were:
		+/-EffectWordNet
		NRC Hashtag Sentiment Lexicon
		Sentiment140 Lexicon
		NRC HashtagEmotion Lexicon\
		
	Performance was poor with sarcasm and short sentences

Prayas at Emoint

Best scoring system at EmoInt 2017
Applied three different deep neural network based models and use an ensemble as final model
Pearson r: 74.7, spearman 73.5

First model: Feed-forward neural network: 
-	tweet representation using word2vec word embeddings ACL W-NUT Shared Task 2015 (400 dimensiosn)
-	TweetToLexiconFeatureVector (43 dimensions)
-	443 dimensions passed into 4 hidden layers, relu activation function for all hidden layers
-	4th hidden layer followed by single sigmoid neuron which predicts intensity between 0 and 1
-	Back-propagation through layers via Mini-batch Gradient Descent
-	Batch size 8, 30 training epochs, Adam optimization
Second model: multitask deep learning
-	Input same 443 dimensional vector
-	The 4 emotions are treated as different subtasks to apply multitask learning
-	Two hidden layers shared between 4 regressors (for all 4 subtasks) and the last 2 layers are allowed tob e different across the differnet subtasks
-	Back-propagation through layers via Mini-batch Gradient Descent (cost function right?)
-	Batch size 8, 30 training epochs, Adam optimization
-	Trained in 4 cycles per epoch (one cycle for each emotion)
Third model: sequence modeling using CNN and LSTMs
-	Input 400 dimensional word vectors (word embeddings), this time they concatenate the word vectors instead of averaging them
-	Fixe length at 50
-	Every tweet (50,400) vector
-	Fed to LSTM or CNN and then to fully connected dense hidden layers
-	The representation learned in the final layer is fed to a single sigmoid neuron which returns intensity between 0 and 1 
-	Relu activation used in hidden layers, dropout set to 0.2
-	Minimizing Mean Absolute Error, optimization through back-propagation through layers via Mini-Batch Gradient Descent with batch size 8, 15 training epochs and Adam optimization
-	CNN best for sadness and anger (2 or 3 out of 3 models of this approach use it), LSTM best for joy and fear (2/3)

Weighted average of model 1, 2, 3 (3 different ones for model 3) so 5 models in total.
Weights determined by cross validation. Weights: 1 for model 1, 3 for model 2, 3 for the 2 best approaches in model 3, 2 for the 3rd best approach of model 3. 
Average score of 75.26% (CV) and 74.70% (test). Beats baseline by 14% and 10% resp.
Of all their models individually, model 3 scored best, followed by model 1 and model 2. 
Suggests that CNNs and LSTMs are better-suited than feed-forward NNs. Note that the ensemble outperforms all of these standalone systems!

IMS at EmoInt2017

They combined the following in a Random Forest Regressor:
1.	Manually created resources
2.	Automatically extended lexicons
3.	The output of a CNN/LSTM NN for sentence regression

The individual features perform similarly (ca. 0.67 on test), but together they get a better correlation (0.72 on test set)
Used Weka Baseline Lexicons (not the other features) 
Lexicon extension created using 3 different approaches: 
- apply supervised method to extend the Weka lexicons to larger Twitter specific vocabulary, 
- learning a new rating score for every word and not just highly associataed terms and, 
- including novel rating categories that provide complementary and potentially useful information, such as valence, arousal, dominance and concreteness.

Tweet regression. CNN/LSTM Neural Network trained on word embeddings (twitter corpus 2016, 50 mil. Tweets) of the tweets (50x300) to predict intensity. 
Input > convolution > max pooling > LSTM

Two handcrafted features: boolean feature if exclamation mark is present in tweet, number of tokens in tweet
Each emotion gets separate model

For the full and final system they combine all features in a random forest regressor using Weka with 800 trees.
The automatically extended corpora (with in-domain twitter embeddings) in combination with the baseline perform well, increase performance in all 4 emotions.

They also use the prediction of other emotions based on tweet as feature (4 extra features), so e.g. a happy tweet might have a sadness intensity of 0.01. 

Takeaway: ensemble, in-domain embeddings, in-domain lexicons, emotion intensity scores as feature



Feature-Enriched Character-Level Convolutions for Text Regression

	20th at EMoInt 2017
	Combined 'engineered features' and character-level information (embeddings) through deep parallel convolution stacks, multi-layer perceptrons and multitask learning

	System was designed for two tasks: EMoInt and Quality estimation. Focussing on Emotion intensity results. The main innovation comes from application of Neural Networks (deep convolutional networks) and multi-task learning. Emphasis on exploiting character-level information from the text.

	System: 
		model is divided in three sections:
			Deep convolution layer: 
			for input tweet, converting to character embeddings
			
			multi-layer perceptron (neural network): 
			for engineered features
			
			multi-layer perceptron: 
			combining information above
			
		One model trained for each emotion, using
			First over the sentiment positivity values from the Stanford Sentiment Treebank until convergence (Stochastic Gradient Descent).
			
			Then over the emotion intensity training sets of the contest until convergence (Mean Squared Error).
			
			Output is Multi-task: both the tweet's emotion intensity and positivity value.
		
	Features:
		Character embeddings: 
		the assumption is that character sequences hold important clues to emotion of text. In this case emojis, exclamation marks etc.
		
		Engineered features: character embeddings
		Authors produced own feature sets (a lexicon of sorts): 
			Minimum, maximum and average positivity of:
			
			single words in the tweet
			
			bigrams in the tweet
			
			trigrams in the tweet
	
	Results:
		Model is outperformed by other strategies, notably those that use more engineered features and external resources. That is, this group used no lexicons!
	
		Still, authors claim character-level features improved peformance scores when dadded to engineered features.
		
		Multi-task learning was deemed a failure, likely due to limited nature of Stanford Sentiment Treebank from which positivity values were drawn.
